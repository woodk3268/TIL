
# 📘 로지스틱 회귀와 정규화 (릿지/라쏘) 이해

---

## ✅ 학습 주제

* 로지스틱 회귀의 개념과 수식
* 로지스틱 회귀에서 정규화의 의미와 적용 방식
* L1/L2 정규화의 차이점 및 실전 적용 시 고려 사항
* "왜 회귀라는 이름인가?"에 대한 원리적 이해

---

## 🧩 핵심 개념 요약

| 개념                   | 설명                                                          |        |                            |
| -------------------- | ----------------------------------------------------------- | ------ | -------------------------- |
| 로지스틱 회귀              | 입력값을 시그모이드 함수로 변환하여, **확률**을 예측하고 이진 **분류**를 수행하는 지도학습 알고리즘 |        |                            |
| 시그모이드 함수             | $\sigma(x) = \frac{1}{1 + e^{-x}}$. 입력을 (0,1) 사이 확률값으로 매핑   |        |                            |
| 정규화 (Regularization) | 모델의 복잡도를 줄이고 일반화 성능을 높이기 위해 **가중치 크기에 패널티**를 주는 기법          |        |                            |
| L2 정규화 (릿지)          | $\lambda \sum \theta^2$. 계수를 작게 유지 (0에 가깝게)                 |        |                            |
| L1 정규화 (라쏘)          | ( \lambda \sum                                              | \theta | ). 일부 계수를 0으로 만들어 변수 선택 효과 |
| 정규화 유무               | 선택 사항. 데이터 규모와 과적합 위험 여부에 따라 유연하게 사용                        |        |                            |

---

## 📐 수식 비교

### 🔹 일반 로지스틱 회귀 손실 함수

$$
J(\theta) = -\sum \left[ y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right]
$$

### 🔹 정규화 추가 시

* **L2 (Ridge)**:

$$
J(\theta) + \lambda \sum \theta_j^2
$$

* **L1 (Lasso)**:

$$
J(\theta) + \lambda \sum |\theta_j|
$$

---

## 💡 실무 적용 인사이트

| 상황               | 정규화 필요 여부 | 이유                  |
| ---------------- | --------- | ------------------- |
| 변수 개수가 많고 과적합 우려 | ✅         | 모델이 과하게 적합되지 않도록 제어 |
| 변수 중요도를 알고 싶을 때  | ✅ (L1)    | 불필요한 변수를 자동 제거      |
| 단순한 모델 + 데이터 충분  | ❌         | 정규화 없이도 안정적 학습 가능   |

```python
# Scikit-learn 예시
from sklearn.linear_model import LogisticRegression

LogisticRegression(penalty='none')       # 정규화 없음
LogisticRegression(penalty='l2', C=1.0)   # 릿지 정규화 (기본)
LogisticRegression(penalty='l1', solver='liblinear', C=1.0)  # 라쏘 정규화
```

---

## 📌 추가 개념: 왜 '회귀'라는 이름인가?

* 로지스틱 회귀는 **클래스를 직접 예측하지 않고**,
  **클래스에 속할 ‘확률(연속값)’을 예측**함 → 회귀적 계산
* 결과적으로 **출력은 확률**, 최종 판단은 **분류**

---

## 🧠 인사이트

* 로지스틱 회귀는 선형 회귀의 출력값을 시그모이드로 감싼 구조.
* 정규화는 단순히 "성능 개선"뿐 아니라, **모델 해석성 확보**나 **데이터 노이즈 대응**에도 필수적인 전략.
* '회귀'는 결국 **연속값을 예측한다는 수학적 의미**이지, 최종 목적이 분류냐 회귀냐와는 다름.


