
# 📚 RNN & Seq2Seq 

---

## ✅ 학습 주제

* RNN의 구조와 작동 원리
* 순환 구조의 의미와 시점별 처리 방식
* Seq2Seq(Sequence-to-Sequence) 모델 등장 배경과 구성
* 고정된 context vector의 한계
* Attention 및 Transformer로의 진화 흐름

---

## 🧩 핵심 개념 요약

| 개념               | 설명                                                           |
| ---------------- | ------------------------------------------------------------ |
| RNN              | 시퀀스(시간 순서) 데이터를 처리하는 순환 신경망. 이전 시점의 출력을 현재 입력에 반영            |
| Hidden State(hₜ) | 이전까지의 정보를 압축한 벡터. 시간에 따라 누적되며 다음 시점에 사용됨                     |
| Weight Sharing   | 모든 시점에서 동일한 가중치 사용 (`Wₓ`, `Wₕ`) → 메모리 절약, 일반화 향상             |
| Seq2Seq          | Encoder-Decoder 구조. 입력 시퀀스를 context vector로 요약 후, 출력 시퀀스를 생성 |
| Context Vector   | 인코더의 마지막 hidden state. 전체 입력 시퀀스를 요약한 벡터                     |
| Attention        | 디코더가 인코더의 모든 hidden state를 동적으로 참조해 더 정확한 출력 생성              |
| Transformer      | RNN 없이 self-attention만으로 입력과 출력 시퀀스를 처리하는 병렬화 가능한 구조         |

---

## 🛠️ RNN 작동 원리

### ✅ 순환 구조

```text
hₜ = tanh(Wₓxₜ + Wₕhₜ₋₁ + b)
yₜ = Wᵧhₜ + bᵧ
```

* **입력 xₜ + 이전 상태 hₜ₋₁ → 현재 상태 hₜ**
* 같은 RNN 셀이 시점마다 **반복 재사용**

---

### ✅ 시간 전개 (Unrolling)

```
x₁ → h₁ → y₁  
x₂ → h₂ → y₂  
x₃ → h₃ → y₃  
```

* 한 샘플이 여러 시간 스텝으로 분해됨
* 각각의 시점에서 RNN이 순차적으로 동작
* 모든 hₜ는 이전 시점의 결과에 의존

---

## 🔄 Seq2Seq 구조와 흐름

### 1. Encoder

```text
x₁ → h₁  
x₂ → h₂  
x₃ → h₃ → context vector
```

* 입력 시퀀스를 처리하고 마지막 hidden state를 **context vector로 추출**

### 2. Decoder

```text
context vector → y₁ → y₂ → y₃ → ... → yₙ
```

* context vector를 바탕으로 **출력 시퀀스를 한 단어씩 생성**
* 출력 종료 조건: `<EOS>` 토큰

---

## 🚨 한계와 개선

| 문제점                | 설명                   |
| ------------------ | -------------------- |
| 고정된 context vector | 문장이 길어질수록 정보 손실 심각   |
| 긴 시퀀스에 취약          | 장기 의존성 문제 (기억이 흐려짐)  |
| 학습 불안정             | 기울기 소실/폭발로 인해 학습 어려움 |
| 순차 처리로 느림          | 병렬화 불가능 (GPU 효율 낮음)  |

---

## 🧠 진화 구조

| 구조              | 핵심 변화                   | 이점              |
| --------------- | ----------------------- | --------------- |
| **LSTM**        | 셀 상태 + 게이트 도입           | 장기 정보 유지 강화     |
| **GRU**         | LSTM 경량 버전              | 계산 효율 + 성능      |
| **Attention**   | 입력 시퀀스 전체를 동적으로 참조      | 문맥 정렬 개선        |
| **Transformer** | RNN 제거 + Self-Attention | 병렬 처리 가능, 성능 향상 |

---

## 💡 인사이트 요약

* RNN은 과거 정보를 누적하여 순차적으로 처리하는 구조로, 시계열 및 텍스트 처리에 강력한 기초 모델
* Seq2Seq 구조는 입력/출력 시퀀스의 **길이 차이 문제를 해결**하기 위해 고안됨
* 고정된 벡터 하나로 모든 의미를 담는 건 비효율적 → **Attention으로 개선**
* 현재는 **Transformer 구조가 표준**이며, GPT, BERT, T5 모두 이 구조 기반

